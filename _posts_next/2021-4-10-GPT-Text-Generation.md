---
layout: post
title: Recent Advances in GPT-based Controlled Text Generation
---

This article provides an overview of recent GPT-based controlled text generation papers.

When [GPT-3](https://arxiv.org/abs/2005.14165) came out in last May, people were instantly amazed by its human-like output and striking versatility.
However, what comes with its superpower of synthesizing text on any topic is that the generated text is often socially non-normative and factually ungrounded, 
which will be a big problem for its real life deployment.
As GPT-3 is not publicly available, in this blog, I survey a couple of papers researching how to learn a controlled text generation system using [GPT-2](https://openai.com/blog/gpt-2-6-month-follow-up/),
a predecessor and light version of GPT-3, to hopefully give us some ideas of fixing the problems in GPT-3.