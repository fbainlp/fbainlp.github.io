---
layout: post
title: A Review of GPT-based Controlled Text Generation Paper
---

When [GPT-3](https://arxiv.org/abs/2005.14165) came out in last May, people were instantly amazed by its human-like output and striking versatility.
However, what comes with its superpower of synthesizing text on any topic is that the genearted text is often socally non-normative and factually ungrounded, which will be a big problem for its real life deplotment.
As GPT-3 is not publically available, in this blog, I survery a couple of papers researching how to learn a controlled text generation system using [GPT-2](https://openai.com/blog/gpt-2-6-month-follow-up/),
a predecessor and light version of GPT-3, to hopefully give us some ideas of fixing the problems in GPT-3.